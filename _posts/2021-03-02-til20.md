---
published: true
layout: single
title:  "[TIL] 21/03/02"
header:
  overlay_image: /images/unsplash-image-1.jpg
  caption: "Photo credit: [**Unsplash**](https://unsplash.com)"
  actions:
    - label: "Learn more"
      url: ""
      
categories: [TIL]
tags: [programmers]
comments: true
---

오늘의 TIL

&nbsp;

&nbsp;

# 추천엔진 

## 넷플릭스 프라이즈

추천 엔진 경진 대회로 여러 추천 엔진들이 등장한 대표적인 추천엔진 대회입니다.
넷플릭스 경진대회의 경우 에러를 측정할 때 RMSE(Root Mean Squre Error)로 평가를 진행했으며, 사용자와 영화 리스트를 짝지어 정답의 지표로 사용했습니다.  

우승팀은 앙상블 방식의 모델을 사용했는데 너무 긴 실행시간 때문에 실제 프로덕션에서는 사용이 불가했습니다. (SVD++)
기존에는 유사도 기반의 추천 방식이 대다수였으나 이 대회 이후로 모델링으로 추천 시스템을 구성하는 것에 대한 가능성이 열리게 됐습니다.  

이후 앙상블 알고리즘이 보편화되고, 딥러닝을 기반으로한 추천이 활기를 띄기 시작했습니다. (오토인코더, 아이템 관련 사용자정보 시간순 인코딩-RNN, SageMaker(DSSNTE) 등)

## 인기도 기반 추천 유닛
- cold start 이슈가 존재하지 않음
- 인기도의 기준 (평점, 매출, 최다 판매 등)
- 사용자 정보에 따라 확장 가능 (ex 서울지역 인기 아이템 추천)
- 개인화되어 있지 않은 경우가 많음 
- 아이템의 분류체계 정보 존재 여부에 따라서 쉽게 확장 가능 
- 인기도를 다른 기준으로 바꿔서 다양한 추천 유닛 생성 가능 

## 텍스트를 벡터로 표현하기 

- Bag of words (카운트, TF-IDF 등)
간단한 방법으로 단어를 행렬의 차원으로 표현할 수 있습니다. bag of words 방식은 문서들에 나타나는 단어수가 n개이면 n차원으로 문서를 표현할 수 있습니다. 

```py
from sklearn.feature_extraction.text import CountVectorizer
# doc1 = sample sentences
text = [doc1,doc2,...]
# 모델정의 
countvectorizer = CountVectorizer(analyzer='word', stopwords='english')
# 데이터에 모델 적용
count_wm = countervectorizer.fit_transform(text)
```

TF-IDF 방식은 다음과 같이 구현할 수 있습니다. 

```py
from sklearn.feature_extraction.text import TfidfVectorizer
# doc1 = sample sentences
text = [doc1,doc2,...]
# 모델정의 
tfidfvectorizer = TfidfVectorizer(analyzer='word', stopwords='english', norm='l2)
# 데이터에 모델 적용
tfidf_wm = tfidfvectorizer.fit_transform(text)
```

추가적으로 코사인 유사도를 통해 문서간의 유사도를 구할 수 있습니다. 

```py
from sklearn.metrics.pairwise import cosine_similarity

cosine_similarities = cosine_similarity(tfidf_wm)
print(cosine_similarities)
```

- TF-IDF 문제점 
정확하게 동일한 단어가 나와야 유사도계산이 이루어집니다. 동의어 처리가 불가합니다.  
단어의 수가 늘어나고 아이템의 수가 늘어나면 연산이 크게 늘어납니다.  

- 보완
워드 임베딩 방식이나 LSA(Latent Semantic Analysis)와 같은 차원 축소방식을 활용하면 위와같은 단점을 보완할 수 있습니다.  





