---
published: true
layout: single
title:  "[TIL] 21/01/19"
header:
  overlay_image: /images/unsplash-image-1.jpg
  caption: "Photo credit: [**Unsplash**](https://unsplash.com)"
  actions:
    - label: "Learn more"
      url: ""
      
categories: [TIL]
tags: [programmers]
comments: true
---

오늘의 TIL

# 크롤링
간단한 프로젝트로 만든 스크래퍼는 페이지의 클래스 네임이 바뀌면 제대로 작동하기 힘들다는 문제점이 있습니다.  
이 문제를 해결하기 위한 조사를 진행하면서 크롤링을 어떻게 하는지 어떤 시기에 반복해서 작동시키는지에 대한 글을 읽을 수 있었습니다.
  
다음과 같은 글을 참고하며 학습했습니다. 
[참고]([https://www.diva-portal.org/smash/get/diva2:1480471/FULLTEXT01.pdf](https://www.diva-portal.org/smash/get/diva2:1480471/FULLTEXT01.pdf))

&nbsp;

# 행렬 곱셈
두 벡터의 내적이 양의 값이면 서로 유사하며, 음의 값이면 서로 반대되는 성질을 갖습니다. 
행렬 곱셈은 기본적으로 벡터의 변환을 야기합니다.  
공간을 변환시켜 학습을 용이하게 만듭니다. 

&nbsp;

# 유사도
벡터의 방향과 크기가 비슷할 수록 유사하다 할수 있습니다.  
공간상에서 비슷한 특징을 유사도로 측정하거나 판단할 수 있습니다. 
예시로는 cos 유사도가 있습니다.  
여러 벡터의 공분산을 통해서 벡터의 요소들이 각각 어느 정도의 분산을 지니고 있는지 알 수 있습니다. 

&nbsp;

# 퍼셉트론 해석
퍼셉트론에서 나온 가중치(weight)들을 이용해 기준 벡터를 만들 수 있습니다.  
이 기준 벡터와 새로운 데이터를 내적하면 기준이 되는 벡터와의 유사도를 구할 수 있습니다.  
유사도와 활성화 함수를 이용해 activation을 구할 수 있으며, 기준 벡터를 바꿔가면서 학습을 진행할 수 있습니다.   

&nbsp;

# 특이값 분해, SVD(singular value decomposition)
여러 잘 쓰여진 글들을 종합한 글을 참고해서 학습했습니다.  
[특이값 분해 관련 포스트](https://bkshin.tistory.com/entry/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D-20-%ED%8A%B9%EC%9D%B4%EA%B0%92-%EB%B6%84%ED%95%B4Singular-Value-Decomposition)

&nbsp;

# 베이즈 정리의 해석
사후확률 => 우도확률 * 사전확률  
posteriori 는 likelihood와 proir에 비례합니다.  
보통 기계학습은 사후확률을 추정하기 힘들 때 사용합니다.  
모수의 매개변수를 찾기 위해서는 관찰된 값을 토대로 최대 우도확률을 구합니다.  

&nbsp;

# 교차 엔트로피 
두 확률분포 사이의 엔트로피 관계를 나타냅니다.  
오차함수로 사용될 수 있으며 주로 딥러닝 방식에서 사용됩니다.  
식을 간단하게 정리해보면 KL 다이버전스를 도출할 수 있는데, 이를 최소화하면 데이터간 분포의 차이를 최소화 시킬 수 있습니다. 

