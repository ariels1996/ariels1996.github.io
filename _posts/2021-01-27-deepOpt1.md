---
published: true
layout: single
title:  "[programmers]심층학습 최적화하기 I"
header:
  overlay_image: /images/unsplash-image-1.jpg
  caption: "Photo credit: [**Unsplash**](https://unsplash.com)"
  actions:
    - label: "Learn more"
      url: ""
         
categories: [Deep Learning]
tags: [programmers, Deep Learning]
comments: true

toc: true
toc_label: "Contents"
toc_sticky: true
# toc_icon: "heart"  # corresponding Font Awesome icon name (without fa prefix)
---

심층학습을 최적화 하기위한 노력들을 함께 알아봅시다.  

# 심층학습 최적화하기 

훈련집합으로 학습을 마친 후, 현장에서 발생하는 새로운 샘플을 잘 예측해야 합니다.  
-> 즉, 일반화 능력이 좋아야 합니다.  

학습을 잘하기 위한 모델은 어떤 조건들을 만족시켜야 할까요?  
지금까지 발전한 모델들이 어떤 문제를 해결함으로써 발전해왔는지 함께 알아보도록 하겠습니다.  

&nbsp;

## 여려운 이유
매개 탐색 공간에서 목적함수는 복잡한 모양을 띄고 있는 non-convex 성질을 지니고 있습니다.  
고차원의 특징 공간을 정확히 정의하는 것은 어려우며, 데이터가 희소할 수록 그 어려움은 증대됩니다.  
또한 훈련시간이 오래 걸리는 단점이 있습니다.  

&nbsp;

# 목적함수를 다시 생각해보기

구하고자하는 목적에 따라 판단의 기준 또한 달라져야 합니다.  
딥러닝에서는 이러한 판단의 기준을 목적함수로 분별합니다.  
성능 판정은 매개변수의 갱신과도 큰 영향이 있기 때문에 목적함수를 잘 정하는 것이 중요합니다.  
&nbsp;

## 평균제곱 오차 

$$e = 1/2 \| y - o \|^{2}_{2}$$

여기서 $o$는 로지스틱 시그모이드 함수를 결과에 적용한 값입니다.  
오차가 클 수록 $e$값이 커지므로 모델의 성능을 평가할 수 있습니다. 

- 딥러닝에서의 단점 
    - 신경망 학습 과정에서 큰 오류임에도 gradient decent가 작게 갱신될 수 있습니다.  

위와 같은 단점은 로지스틱 시그모이드 함수의 특성 때문에 나오는 결과입니다.  
시그모이드 함수의 도함수를 살펴보면 절댓값이 커질수록 경사도가 작아지는 특성을 지닙니다.  
따라서 local gradient에서 특성을 온전히 전달하기 힘든 구조라고 할 수 있습니다.  

목적함수를 바꾸거나, 활성화함수를 바꿔줄 필요성이 있습니다.  
&nbsp;

## 교차 엔트로피

$$BCE(x)=−1/N\sum\limits_{i=1}^N y_{i}\log(h(x_i;\theta))+(1−y_i)\log(1−h(x_i;\theta))$$

잘못된 학습으로 오분류된 손실은 큰 값의 loss를 반환해주고, 제대로 분류된 손실은 0에 인접한 값을 내는 것을 확인할 수 있습니다.  

시그모이드 함수의 출력값을 확률과 같이 sum이 1인 값으로 바꿔주기 위해 softmax함수를 활성화 함수 뒤에 붙여줍니다.  

## 음의 로그 우도함수 (NLL)

하나의 클래스로만 분류되어야 한다면 음의 로그우도 함수를 사용할 수 있습니다.  
이것은 정답에 해당하는 loss만 추출하는 함수입니다.  

$$ e = -\log_{2}o_y

- 소프트맥스와 로그우도 
    - 소프트맥스는 최댓값이 아닌 값을 억제해서 0에 가깝게 만든다는 의도를 내포합니다.  
    - 정답에 해당하는 노드만 보겠다는 로그우도와 잘 어울립니다.  
    - 둘을 결합해서 사용하는 경우가 많습니다.  

## 결론
신경망의 가중치 갱신은 loss function의 영향을 크게 받습니다.  
따라서 목적에 맞는 loss를 잘 설정하고, loss function의 특성을 잘 이해해야합니다.  

&nbsp;

# 데이터 전처리 

## 규모문제

예시로 데이터의 규모가 학습에 어떤 영향을 주는지 알아봅시다.  
키 데이터에서 1.8m 와 1.5m는 차이가 0.3이 난다고 합시다.  
이어서 몸무게 데이터에서 70kg 와 50kg 라고 했을 때 차이는 20kg 차이가 나게 됩니다. 

두 데이터의 규모는 약 100배의 차이가 나게 됩니다.  
특징들에 연결된 가중치가 규모에 따라 학습속도가 달라질 수 있습니다.  
따라서 데이터간 규모를 맞추는 것은 학습을 돕는 요소 중 하나로 작용될 수 있습니다. 

## 모든 특징이 양수일 경우 

특징이 모두 양수라면 각각의 노드에 대해서 모두 같은 gradient의 방향으로 학습될 가능성이 있습니다.  
각각의 결과에서 가중치들이 뭉치로 증가 또는 감소하기 때문에 수렴이 느려질 수 있습니다.  

## 정규화 
이러한 문제를 해결하기 위해 정규화를 사용합니다.  
평균이 0이고 표준편차가 1이 되도록 만듭니다.  
$$x^{new}_{i} = {x^{old}_{i} - \mu_i} / {\sigma_i} $$

## 명목변수(카테고리 데이터)
카테고리 관련 데이터는 크고 작다는 개념으로 나눌 수 없습니다.  
따라서 one-hot 인코딩을 통해 각 카테고리가 따로 input으로 들어갈 수 있도록 만들어줘야 합니다.  


&nbsp;

# 가중치 초기화

## 대칭적 가중치 문제
가중치들이 같은 값을 지니고 있을 경우 학습을 진행해도 같은 변화를 받아 여러 노드가 중복되는 일이 발생합니다.  
이런 일이 발생하면 다양한 특징을 학습할 수 없게 됩니다.  

이를 해결하기 위해, 신경망을 난수로 초기화 함으로써 대칭성을 파괴하는 아이디어를 생각해볼 수 있습니다.(symmetric break)

난수는 가우시안 또는 균일 분포에서 추출합니다.  
보통 난수의 범위는 노드로 들어오는 edge의 갯수를 이용합니다.  

이러한 가중치를 적절하게 정하는 노력은 배치 정규화에서도 볼 수 있습니다.  

&nbsp;

# 가속도, 관성

## 경사도의 잡음 현상 
잡음 현상은 gradient가 학습에 정도에 따라서 여러 방향으로 튀게 되는 현상을 의미합니다.  
full batch gradient decent처럼 부드럽게 학습시키고 싶다는 생각에서 잡음을 줄이고자 하는 노력들이 있었습니다.  
학습에 momentum을 주면 경사도가 갑자기 변하는 것을 줄일 수 있습니다.  
이를 활용하면 지역 최소 혹은 안장점에 빠지는 문제를 해소할 수 있습니다.  

momentum v는 이전 경사도를 누적한 것에 해당합니다.  

## 네스테로프 가속 경사도 관성 

현재 v 값으로 다음 이동할 곳을 예측한 후에 그 곳에서 예측되는 그래디언트를 이용하는 방법입니다.  




