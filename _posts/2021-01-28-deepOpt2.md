---
published: true
layout: single
title:  "[programmers]심층학습 최적화하기 II"
header:
  overlay_image: /images/unsplash-image-1.jpg
  caption: "Photo credit: [**Unsplash**](https://unsplash.com)"
  actions:
    - label: "Learn more"
      url: ""
         
categories: [Deep Learning]
tags: [programmers, Deep Learning]
comments: true

toc: true
toc_label: "Contents"
toc_sticky: true
# toc_icon: "heart"  # corresponding Font Awesome icon name (without fa prefix)
---

저번 포스트에 이어 심층학습을 최적화하는 방법들을 알아봅시다.  

&nbsp;

&nbsp;

# 적응적 학습률

기존의 경사도 갱신은 모든 매개변수에 대해 같은 크기의 학습률을 사용했습니다.  
적응적 학습률은 매개변수마다 자신의 상황에 따라 학습률을 조절해가면서 사용합니다.  

## AdaGrid (adaptive gradient)

learning rate를 조절하기 위해서 과거에 있던 gradient의 요소별 곱을 이용합니다.  
이전 경사도의 누적률이 커질수록 갱신값은 작아져서 조금만 이동하게 됩니다.

## RMSProp

AdaGrad의 단점은 단순히 경사도의 제곱을 이용한다는 것이었습니다.  
이러한 경우 오래된 경사도와 최근 경사도의 비중이 같아지고 learning rate는 계속 작아지면서 수렴을 방해할 가능성이 있습니다. 
오래된 경사도와 최근 경사도의 비중을 다르게 줘서 이 문제를 해결하고자 한 방식이 RMSProp 입니다.  

## Adam(adaptive moment)

RMSProp에 관성을 추가로 적용한 알고리즘 입니다.  


&nbsp;

# 활성함수

## ReLU(Recified Linear Unit)
sigmoid 함수는 활성값이 커지면 포화상태가 되고 경사도가 0에 가까운 값을 출력합니다.  
이는 매개변수 갱신을 느리게 만드는 요인으로 작용합니다.  
이를 해결하기 위해 ReLU(Recified Linear Unit)이라는 새로운 활성함수를 도입했습니다.  

다만 ReLU는 0보다 작은 값들의 gradient 갱신을 진행하지 않습니다.  
이후 0보다 작은 값들을 보정해주기 위해 ELU와 같은 활성함수가 등장하지만 연산량이 높아지기 때문에 사용하기 이전에 잘 생각해 보아야 합니다.  

&nbsp;

# 배치 정규화 

## 공변량 변화 현상

훈련집합과 테스트집합의 분포가 다를 경우 생기는 문제들을 의미합니다.  

처음 데이터의 분포를 정규화를 통해 잘 만들었더라도, 가충치에 의해서 학습을 통해 분포가 점점 변하게 됩니다.  
데이터의 분포가 수시로 바뀌기 때문에 학습을 방해하는 요인으로 작용할 수 있습니다. 

## 배치 정규화  

공변량 변화 현상을 누그러뜨리기 위해 정규화를 층 단위로 적용하는 기법입니다.  

정규화는 선형함수 이후 비선형함수를 적용하기 이전에 적용하는 것이 좋습니다.  
샘플을 나눠서 학습하는 미니배치에 좀 더 적합한 방법입니다.  

정규화를 진행하는데 사용되는 평균과 분산은 미니배치 단위로 계산해서 나온 선형식의 결과 $z$들을 사용합니다. 

장점
- 신경망의 경사도 흐름 개선
- 높은 학습률 허용
- 초기화에 대한 의존성 감소
- 규제와 유사한 행동을 하면서, 드롭아웃의 필요성을 감소시킴 

CNN에서는 노드 단위가 아닌 특징 맵 단위로 적용합니다. 

&nbsp;

# 규제 

대부분 가지고 있는 데이터에 비해 훨씬 큰 용량의 모델을 사용합니다.   
모델용량에 비해 데이터가 부족한경우의 부족조건문제를 푸는 방법이 필요합니다.  
부족한 학습조건을 제약조건으로 보강하려는 시도가 규제라 할 수 있습니다.  

대표적인 규제기법으로 통계에서는 릿지 회귀(ridge regression), 기계학습에서는 가중치 감쇄 등이 있습니다.  

## 명시적 규제와 암시적 규제

- 명시적 규제  
  가중치 감쇠나 드롭아웃처럼 목적함수나 신경망 구조를 직접 수정하는 방식
- 암시적 규제  
  조기 멈춤, 데이터 증대, 잡음 추가, 앙상블처럼 간접적으로 영향을 미치는 방법

## 가중치 벌칙 

규제항을 통해 매개변수를 작은 값으로 유지시킵니다.  
작은 가중치를 유지하면 모델의 용량을 제한하는 효과를 볼 수 있습니다.  

L2 norm 이나 L1 norm을 대표적으로 사용합니다.  

## 조기 멈춤 

학습을 진행하면 할 수록 bias가 줄어드는 것 보다 variance가 점점 커지게 됩니다.  
이 때문에 validation set의 오류가 최저인 점에서 학습을 멈추자는 아이디어 입니다. 

## 데이터 확대 

과잉적합을 방지하는 가장 확실한 방법은 큰 훈련집합을 사용하는 것입니다.  
실제 데이터를 수집하는데에는 한계가 있으므로 데이터를 인위적으로 변형시켜서 새로운 데이터를 만들어 내는 기법입니다.  


## 드롭아웃 

완전 연결층의 노드 중 일정 비율을 임의 선택하여 제거하는 기법입니다.  
학습은 남는 부분에 대해 진행하게 됩니다.  
다양한 모양의 신경망들을 만들고,  앙상블 결합하는 기법으로 볼 수 있습니다.  

## 앙상블 

서로다른 여러개의 모델을 결합하여 일반화 오류를 줄이는 기법을 의미합니다.  

### 배깅(bootstrap aggregating)
훈련집합을 여러번 샘플링하여 서로 다른 훈련집합을 구성합니다.  

### 부스팅(boosting) 
i 번째 예측기가 틀린 샘플을 i+1번째 예측기가 잘 인식하도록 연계성을 고려합니다.  

### 모델 평균

배깅과 부스팅등으로 학습된 예측기를 결합해야 합니다. 모델들의 평균을 구하거나 투표하여 최종결과를 구합니다.  

&nbsp;

# 하이퍼 파라미터 최적화 

학습 모델에는 내부 파라미터와 가중치라는 두 종류의 매개변수가 있습니다.  
위와 같이 학습에 의해 결정되는 매개변수가 있는 반면 사람이 직접 정하는 파라미터들도 존재합니다. 이를 하이퍼 파라미터라고 부릅니다.  
레이어의 갯수 혹은 learning rate 등이 있습니다.  

최적의 파라미터를 구하기 위해 하이퍼 파라미터들로 조합을 생성한 후 하나씩 학습을 수행합니다.  
이 중 성능이 가장 좋은 모델의 하이퍼 파라미터를 선택하게 됩니다.  

단 매개변수가 많아질수록 수많은 조합이 존재하게 되므로 차원의 저주 문제가 발생합니다.  
따라서 임의 탐색을 하는 것이 유리하며, 경향성이 파악될 경우 그 안에서 세밀하게 튜닝하는 것이 훨씬 좋습니다.  

&nbsp;

# 2차 미분을 이용한 방법 

지금까지는 주어진 식의 1차 미분 결과(gradient)만 살펴보고 경향을 파악했습니다.  
하지만 연구자들은 더 좋은 학습법을 발견하고자 했고 경사하강법의 단점을 보완하는 두가지 개선책에 대해 연구를 진행했습니다.  
현재는 경사하강법을 더 빠르게 진행시키고자 1차 미분을 개선한 경사도의 잡음을 줄이고자 하는 방법을 차용하고 있으며 있으며, 2차미분 정보를 활용하는 방법이 연구되고 었습니다.  

## 뉴턴 방법 
테일러 급수를 활용해서 그레디언트와 헤시언 행렬을 구해 연산을 진행합니다.  
하지만 매개변수의 개수를 m이라 할 때 $O(m^3)$이라는 큰 연산이 필요하기 때문에 대안으로 켤레 경사도 방법이 나오게 됩니다.  

## 켤레 경사도 방법
켤레 경사도는 초기의 점에서 그래디언트를 적용한 최적화 값을 잇는 하나의 line을 활용합니다.  
이 line의 법선을 활용하여 다음 값을 예측하고 현재의 방향을 더해 예측합니다.  

## 유사 뉴턴 방법
헤시안 행렬을 구하기 보다 역행렬을 근사하는 행렬 M을 사용합니다.  
대표적으로 점진적으로 헤시안을 근사화하는 LFGS가 많이 사용됩니다.  
여기서 발전된 메모리를 더 적게쓰는 방법인 L_BFGS가 현대 연구에서 많이 사용되고 있습니다.
